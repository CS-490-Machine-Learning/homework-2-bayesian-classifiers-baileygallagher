{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center><font size=6>CS490: Machine Learning<br>Homework 1</font></center>**\n",
    "\n",
    "# Naive Bayes Classifier\n",
    "<hr>\n",
    "\n",
    "In order to reduce my email load, I decide to implement a machine learning algorithm to decide whether or not I should read an email, or simply file it away instead.  To train my model, I obtain the following data set of binary-valued features about each email,\n",
    "including whether I know the author or not, whether the email is long or short, and whether it has any of several key words, along\n",
    "with my final decision about whether to read it (y=+1 for \"read\", y=-1 for \"discard\").\n",
    "\n",
    "| know author? | is long? | has 'research' | has 'grade' | has 'lottery' | read? |\n",
    "|--------------|----------|----------------|-------------|---------------|-------|\n",
    "| 0 | 0 | 1 | 1 | 0 | -1 |\n",
    "| 1 | 1 | 0 | 1 | 0 | -1 |\n",
    "| 0 | 1 | 1 | 1 | 1 | -1 |\n",
    "| 1 | 1 | 1 | 1 | 0 | -1 |\n",
    "| 0 | 1 | 0 | 0 | 0 | -1 |\n",
    "| 1 | 0 | 1 | 1 | 1 | 1 |\n",
    "| 0 | 0 | 1 | 0 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 0 | 0 | 1 |\n",
    "| 1 | 0 | 1 | 1 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 1 | 1 | -1 |\n",
    "\n",
    "I decide to try a naive Bayes classifier to make my decisions and compute my uncertainty.  In the case of any ties where both classes have equal probability, we will prefer to predict class +1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Naive Bayes prior and likelihoods\n",
    "\n",
    "Compute all the probabilities necessary for a discrete Naive Bayes classifier, i.e.,\n",
    "the class probability p(y) and all the individual feature probabilities p(x_i|y), for each class y and feature x_i. \n",
    "\n",
    "**NOTE: There is not an mltools utility for this--you need to do the computations on your own. However, you can do the counting by hand and just fill in the fractions for each parameter. For example, if in the data above you count that the parameter for p(x_i | y=1) = 2/3, you will just create a a variable below and set it equal to the fraction:**\n",
    "\n",
    "```p_xi_y1 = 2.0 / 3```\n",
    "\n",
    "You will want these variables to be floating points for use below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate your naive Bayes parameters here \n",
    "\n",
    "# Priors\n",
    "# where y=1 (read email)\n",
    "p_ypos = 4.0/10 \n",
    "p_yneg = 1 - p_ypos\n",
    "\n",
    "# Conditional Probabilities\n",
    "# where y=1 (read email)\n",
    "p_x1_y1 =  3.0/4\n",
    "p_x2_y1 = 0.0/4\n",
    "p_x3_y1 = 3.0/4\n",
    "p_x4_y1= 2.0/4\n",
    "p_x5_y1 = 1.0/4\n",
    "\n",
    "# where y=-1, discarded as spam \n",
    "p_x1_y0 = 3.0/6\n",
    "p_x2_y0= 5.0/6\n",
    "p_x3_y0 = 4.0/6\n",
    "p_x4_y0 = 5.0/6\n",
    "p_x5_y0 = 2.0/6\n",
    "\n",
    "p_x1 = 6.0/10\n",
    "p_x2 = 5.0/10\n",
    "p_x3 = 7.0/10\n",
    "p_x4 = 7.0/10\n",
    "p_x5 = 3.0/10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Prediction\n",
    "Use your learned parameters above to predict whether emails with the following features would be kept or discarded:\n",
    "\n",
    "1. Short email from an unknown sender that isn't about research, grades, or a lottery. \n",
    "2. ***x*** = (1, 1, 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROB ONE KEEP  0.009375000000000001\n",
      "PROB ONE TOSS  0.0018518518518518515\n",
      "PROB TWO KEEP:  0.0\n",
      "PROB TWO TOSS:  0.046296296296296315\n"
     ]
    }
   ],
   "source": [
    "#TODO: Compute the predictions for the above email features.\n",
    "#Short email from an unknown sender that isn't about research, grades, or a lottery. \n",
    "# where x1=0, x2=0, x3=0, x4=0, x5 =0\n",
    "# find y = 1 when all x's are 0\n",
    "# p(y=1 | x1=0, x2=0, x3=0, x4=0, x5=0) =   \n",
    "kept_one = (1-p_x1_y1) * (1-p_x2_y1) * (1-p_x3_y1) * (1-p_x4_y1) * (1-p_x5_y1) * p_ypos\n",
    "discarded_one = p_yneg * (1-p_x1_y0) * (1-p_x2_y0) * (1-p_x3_y0) * (1-p_x4_y0) * (1-p_x5_y0)\n",
    "kept_one_x1 =  (1-p_x2_y1) * (1-p_x3_y1) * (1-p_x4_y1) * (1-p_x5_y1) * p_ypos\n",
    "print(\"PROB ONE KEEP \", kept_one)\n",
    "print(\"PROB ONE TOSS \", discarded_one)\n",
    "print(\"PROB ONE KEEP REMOVED X1\", kept_one_x1)\n",
    "### x = (1,1,0,1,0)\n",
    "keep_two = p_x1_y1 * p_x2_y1 * (1-p_x3_y1) * p_x4_y1 * (1-p_x5_y1) * p_ypos\n",
    "discard_two = p_yneg * p_x1_y0 * p_x2_y0 * (1-p_x3_y0) * p_x4_y0 * (1-p_x5_y0)\n",
    "discard_two_x1 = p_yneg  * p_x2_y0 * (1-p_x3_y0) * p_x4_y0 * (1-p_x5_y0)\n",
    "print(\"PROB TWO KEEP: \", keep_two)\n",
    "print(\"PROB TWO TOSS: \", discard_two)\n",
    "print(\"PROB TWO TOSS: \", discard_two_x1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In problem one, we should keep the email. In problem two we should throw out the email. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Joint versus Naive Bayes\n",
    "**Why should we probably not use a \"joint\" Bayes classifier (using the joint probability of the features x,\n",
    "as opposed to the conditional independencies assumed by Naive Bayes) for these data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Naive Bayes helps us to keep our products under control. If we added another X value, our amount of multiplication would only increase by one. If we were to use a joint Bayes, it would scale up exponentially, which makes it get out of control quickly and is difficult to work with, and if it does work, it is very slow. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Classification with Missing Features\n",
    "\n",
    "**Suppose that before we make our predictions, we lose access to my address book, so that we cannot tell whether the email author is known.  Do we need to re-train the model to classify based solely on the other four features?  If so, how? What, if anything, changes about the parameters or the way they are used?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In naive bayes, little would change with the removal of one paramater. We would just compute our decision based on the four exisitng x-values multiples by our Y. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
